{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X9FCRoV3e-_1"
      },
      "source": [
        "# **Análisis de componentes principales**\n",
        "###**Resolución**\n",
        "###**Apellidos y nombres: Dorado Torres Diego Alonso**\n",
        "###**Código: 170594**\n",
        "\n",
        "#### Este notebook retoma el tema del análisis exploratorio a través del concepto de aprendizaje no supervisado conocido como análisis de componentes principales (PCA). Usaremos el conjunto de datos [Million Song Dataset](http://labrosa.ee.columbia.edu/millionsong/) del [UCI Machine Learning Repository](https://archive.ics.uci.edu/ml/datasets /YearPredictionMSD) ) utilizado en Lab4a.\n",
        "\n",
        "#### **En este notebook:**\n",
        " #### *Parte 1:* Tutorial de PCA en un conjunto de datos artificial\n",
        "   #### *Vista 1:* Gaussianas bidimensionales\n",
        "   #### *Parte 2:* Función PCA para aplicar en un RDD\n",
        "   #### *Vista 2:* Proyección PCA\n",
        " #### *Parte 3:* Aplicación de PCA sobre la base Million Song, con aplicación de Regresión Lineal sobre la base proyectada\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wq0I5WiKe-_5"
      },
      "source": [
        " \n",
        "### **Parte 1: Passo a passo do PCA em um dataset artificial**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5f-awB45e-_6"
      },
      "source": [
        "#### **Vista 1: Gaussianas bidimensionales**\n",
        "\n",
        "#### El Análisis de Componentes Principales, o PCA, es una estrategia para la reducción de la dimensionalidad. Para entender cómo funciona PCA, usemos una base de datos generada artificialmente usando una [distribución gaussiana bidimensional] (http://en.wikipedia.org/wiki/Multivariate_normal_distribution). Esta distribución tiene como parámetros la media y la varianza de cada dimensión, así como la covarianza entre dimensiones.\n",
        " \n",
        "#### Para nuestros experimentos, estableceremos la media de cada dimensión en 50 y la varianza en 1. Y probaremos dos valores de covarianza: 0 y 0,9. Cuando la covarianza es cero significa que las variables no tienen correlación y los datos son esféricos. Cuando establecemos la covarianza en 0,9, significa que las dos dimensiones tienen una correlación positiva y fuerte, es decir, cuando una variable aumenta su valor, la otra tiende a aumentar también. Cuando tenemos una fuerte correlación significa que podemos representar dos o más variables como una sola, veremos que PCA funciona muy bien para estos casos."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "E6f9avbue-_6"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "def preparePlot(xticks, yticks, figsize=(10.5, 6), hideLabels=False, gridColor='#999999',\n",
        "                gridWidth=1.0):\n",
        "    \"\"\"Template for generating the plot layout.\"\"\"\n",
        "    plt.close()\n",
        "    fig, ax = plt.subplots(figsize=figsize, facecolor='white', edgecolor='white')\n",
        "    ax.axes.tick_params(labelcolor='#999999', labelsize='10')\n",
        "    for axis, ticks in [(ax.get_xaxis(), xticks), (ax.get_yaxis(), yticks)]:\n",
        "        axis.set_ticks_position('none')\n",
        "        axis.set_ticks(ticks)\n",
        "        axis.label.set_color('#999999')\n",
        "        if hideLabels: axis.set_ticklabels([])\n",
        "    plt.grid(color=gridColor, linewidth=gridWidth, linestyle='-')\n",
        "    map(lambda position: ax.spines[position].set_visible(False), ['bottom', 'top', 'left', 'right'])\n",
        "    return fig, ax\n",
        "\n",
        "def create2DGaussian(mn, sigma, cov, n):\n",
        "    \"\"\"Randomly sample points from a two-dimensional Gaussian distribution\"\"\"\n",
        "    np.random.seed(142)\n",
        "    return np.random.multivariate_normal(np.array([mn, mn]), np.array([[sigma, cov], [cov, sigma]]), n)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qY9o0Rv3e-_8"
      },
      "outputs": [],
      "source": [
        "dataRandom = create2DGaussian(mn=50, sigma=1, cov=0, n=100)\n",
        "\n",
        "# generate layout and plot data\n",
        "fig, ax = preparePlot(np.arange(46, 55, 2), np.arange(46, 55, 2))\n",
        "ax.set_xlabel(r'Simulated $x_1$ values'), ax.set_ylabel(r'Simulated $x_2$ values')\n",
        "ax.set_xlim(45, 54.5), ax.set_ylim(45, 54.5)\n",
        "plt.scatter(dataRandom[:,0], dataRandom[:,1], s=14**2, c='#d6ebf2', edgecolors='#8cbfd0', alpha=0.75)\n",
        "pass"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lFKdQ-yge-_8"
      },
      "outputs": [],
      "source": [
        "dataCorrelated = create2DGaussian(mn=50, sigma=1, cov=.9, n=100)\n",
        "\n",
        "# generate layout and plot data\n",
        "fig, ax = preparePlot(np.arange(46, 55, 2), np.arange(46, 55, 2))\n",
        "ax.set_xlabel(r'Simulated $x_1$ values'), ax.set_ylabel(r'Simulated $x_2$ values')\n",
        "ax.set_xlim(45.5, 54.5), ax.set_ylim(45.5, 54.5)\n",
        "plt.scatter(dataCorrelated[:,0], dataCorrelated[:,1], s=14**2, c='#d6ebf2',\n",
        "            edgecolors='#8cbfd0', alpha=0.75)\n",
        "pass"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O-qFEWVYe-_9"
      },
      "source": [
        "#### **(1a) Interpretación del PCA**\n",
        "\n",
        "#### PCA puede interpretarse como la identificación de las direcciones en las que los datos varían más. En el primer paso de PCA, necesitamos centralizar nuestros datos. El primer paso requiere que calculemos el promedio de cada atributo (columna) en la base de datos. Luego, para cada observación (fila), modifique los valores de los atributos por la media correspondiente. De esta forma tendremos una base de datos en la que cada atributo tiene una media de cero.\n",
        "\n",
        "#### Tenga en cuenta que `correlatedData` es un RDD de arreglos NumPy. Esto nos permite hacer algunas operaciones más fácilmente. Por ejemplo, si usamos la transformación `sum()`, PySpark hará la suma de los valores de cada columna."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nIShQRZ-e-_-"
      },
      "outputs": [],
      "source": [
        "# EXERCICIO\n",
        "correlatedData = sc.parallelize(dataCorrelated)\n",
        "\n",
        "meanCorrelated = correlatedData.mean()     #Solución\n",
        "correlatedDataZeroMean = correlatedData.map(lambda x: x - meanCorrelated )    #Solución\n",
        "\n",
        "print meanCorrelated\n",
        "print correlatedData.take(1)\n",
        "print correlatedDataZeroMean.take(1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Yj1ZGOjje-_-"
      },
      "outputs": [],
      "source": [
        "# TEST Interpreting PCA (1a)\n",
        "from test_helper import Test\n",
        "Test.assertTrue(np.allclose(meanCorrelated, [49.95739037, 49.97180477]),\n",
        "                'incorrect value for meanCorrelated')\n",
        "Test.assertTrue(np.allclose(correlatedDataZeroMean.take(1)[0], [-0.28561917, 0.10351492]),\n",
        "                'incorrect value for correlatedDataZeroMean')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PeYlnYHJe-__"
      },
      "source": [
        "#### **(1b) Matriz de covarianza**\n",
        "\n",
        "#### Calculemos la matriz de covarianza de nuestros datos. Si definimos $\\scriptsize \\mathbf{X} \\in \\mathbb{R}^{n \\times d}$ como la matriz de datos centrada en cero, entonces la matriz de covarianza se define como: $$ \\mathbf{C } _{\\mathbf X} = \\frac{1}{n} \\mathbf{X}^\\top \\mathbf{X} \\,.$$ Esta matriz se puede calcular calculando el producto vectorial de cada fila consigo misma, en luego realizar la suma de las matrices resultantes y, finalmente, dividir por el número total de objetos en la base de datos. Los datos son bidimensionales, por lo que la matriz de covarianza es una matriz de 2x2.\n",
        " \n",
        "#### Tenga en cuenta que [np.outer()](http://docs.scipy.org/doc/numpy/reference/generated/numpy.outer.html) se puede usar para calcular el producto externo de dos arreglos NumPy ."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vcCy5TAge-__"
      },
      "outputs": [],
      "source": [
        "# EXERCICIO\n",
        "# Compute the covariance matrix using outer products and correlatedDataZeroMean\n",
        "correlatedCov = (correlatedDataZeroMean.map(lambda x: np.outer(np.transpose(x),x)).sum()/correlatedDataZeroMean.count())     #Solución\n",
        "print correlatedCov"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "z6wFxMHEe_AA"
      },
      "outputs": [],
      "source": [
        "# TEST Sample covariance matrix (1b)\n",
        "covResult = [[ 0.99558386,  0.90148989], [0.90148989, 1.08607497]]\n",
        "Test.assertTrue(np.allclose(covResult, correlatedCov), 'incorrect value for correlatedCov')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_rZ7sB1ae_AA"
      },
      "source": [
        "#### **(1c) Función de covarianza**\n",
        "\n",
        "#### Utilizando las expresiones del ejercicio anterior, escribe una función que calcule la matriz de covarianza de un RDD arbitrario. Tenga en cuenta que los datos deben estar centrados para que tengan una media cero."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KXmhxBJXe_AA"
      },
      "outputs": [],
      "source": [
        "# EXERCICIO\n",
        "def estimateCovariance(data):\n",
        "    \"\"\"Compute the covariance matrix for a given rdd.\n",
        "\n",
        "    Note:\n",
        "        The multi-dimensional covariance array should be calculated using outer products.  Don't\n",
        "        forget to normalize the data by first subtracting the mean.\n",
        "\n",
        "    Args:\n",
        "        data (RDD of np.ndarray):  An `RDD` consisting of NumPy arrays.\n",
        "\n",
        "    Returns:\n",
        "        np.ndarray: A multi-dimensional array where the number of rows and columns both equal the\n",
        "            length of the arrays in the input `RDD`.\n",
        "    \"\"\"\n",
        "    meanVal = data.mean() #Solución\n",
        "    data_ZeroMean = data.map(lambda x: x - meanVal)\n",
        "    return data_ZeroMean.map(lambda x: np.outer(np.transpose(x),x)).sum()/data_ZeroMean.count()\n",
        "\n",
        "correlatedCovAuto= estimateCovariance(correlatedData)\n",
        "print correlatedCovAuto"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "j-_yXAdee_AB"
      },
      "outputs": [],
      "source": [
        "# TEST Covariance function (1c)\n",
        "correctCov = [[ 0.99558386,  0.90148989], [0.90148989, 1.08607497]]\n",
        "Test.assertTrue(np.allclose(correctCov, correlatedCovAuto),\n",
        "                'incorrect value for correlatedCovAuto')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8t7JEe5ae_AB"
      },
      "source": [
        "#### **(1d) Valores propios y vectores propios**\n",
        "\n",
        "#### Ahora que tenemos la matriz de covarianza, podemos usarla para encontrar las direcciones de mayor variación en los datos. Los valores propios y los vectores propios nos traen dicha información. Los $\\scriptsize d$ vectores propios de la matriz de covarianza nos dan las direcciones de mayor varianza y se denominan componentes principales. Los valores propios asociados son las varianzas en estas direcciones. En particular, el atuovector correspondiente al valor propio más grande es la dirección de máxima varianza. El cálculo de los autovalores y autovectores de la matriz de covarianza tiene una complejidad de aproximadamente $O(d^3)$ para una matriz d x d. Cuando nuestro $d$ es lo suficientemente pequeño, podemos calcular los valores propios y los vectores propios localmente.\n",
        " \n",
        "#### Use la función [eigh](http://docs.scipy.org/doc/numpy/reference/generated/numpy.linalg.eigh.html) de la biblioteca `numpy.linalg` para calcular los valores propios ​y vectores propios. Luego ordene los vectores propios según los valores propios, de mayor a menor, generando una matriz donde los vectores propios son las columnas (y la primera columna es el vector propio más grande).\n",
        "\n",
        "#### Tenga en cuenta que la función [np.argsort](http://docs.scipy.org/doc/numpy/reference/generated/numpy.argsort.html#numpy-argsort) puede usarse para obtener el orden de índices de elementos en orden ascendente o descendente. Finalmente, coloque el vector propio más grande en la variable `topComponent`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "z6_lFByWe_AB"
      },
      "outputs": [],
      "source": [
        "# EXERCICIO\n",
        "from numpy.linalg import eigh\n",
        "\n",
        "# Calculate the eigenvalues and eigenvectors from correlatedCovAuto\n",
        "eigVals, eigVecs = np.linalg.eigh(correlatedCovAuto)         #Solución\n",
        "print ('eigenvalues: {0}'.format(eigVals))\n",
        "print ('\\neigenvectors: \\n{0}'.format(eigVecs))\n",
        "\n",
        "# Use np.argsort to find the top eigenvector based on the largest eigenvalue\n",
        "inds = np.argsort(eigVals)    #Solución\n",
        "topComponent = eigVecs[:,inds.tolist().index(len(eigVals)-1)]     #Solución\n",
        "print ('\\ntop principal component: {0}'.format(topComponent))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eL1m7EHDe_AC"
      },
      "outputs": [],
      "source": [
        "# TEST Eigendecomposition (1d)\n",
        "def checkBasis(vectors, correct):\n",
        "    return np.allclose(vectors, correct) or np.allclose(np.negative(vectors), correct)\n",
        "Test.assertTrue(checkBasis(topComponent, [0.68915649, 0.72461254]),\n",
        "                'incorrect value for topComponent')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d6JPl8V5e_AC"
      },
      "source": [
        "#### **(1e) puntajes PCA**\n",
        "\n",
        "#### Calculamos los componentes principales para una base de datos no esférica. Ahora usemos este componente principal para derivar una representación unidimensional de los datos originales. Para calcular la representación compacta, que se llama puntajes PCA, calcule el producto interno entre cada objeto en la base original y el componente principal."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "j58B0LS0e_AC"
      },
      "outputs": [],
      "source": [
        "# EXERCICIO\n",
        "# Use the topComponent and the data from correlatedData to generate PCA scores\n",
        "correlatedDataScores = correlatedData.map(lambda x: np.dot(x,topComponent))      #Solución\n",
        "print ('one-dimensional data (first three):\\n{0}'.format(np.asarray(correlatedDataScores.take(3))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8PXqyCjLe_AC"
      },
      "outputs": [],
      "source": [
        "# TEST PCA Scores (1e)\n",
        "firstThree = [70.51682806, 69.30622356, 71.13588168]\n",
        "Test.assertTrue(checkBasis(correlatedDataScores.take(3), firstThree),\n",
        "                'incorrect value for correlatedDataScores')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s3M3mqzde_AD"
      },
      "source": [
        "### **Parte 2: Función PCA para aplicar a un RDD**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TnDsuoMKe_AD"
      },
      "source": [
        "#### **(2a) Función PCA**\n",
        "\n",
        "#### Ahora tenemos todos los ingredientes para calcular una función PCA. Nuestra función debe ser genérica para calcular los componentes principales de $k$ y las puntuaciones de PCA. Escriba la función `pca` genérica y pruebe usando `correlatedData` y $\\scriptsize k = 2$. Sugerencia: Utilice los resultados de la Parte (1c), la Parte (1d) y la Parte (1e).\n",
        " \n",
        "#### Nota: Como se discutió anteriormente, esta implementación es eficiente mientras $\\scriptsize d$ es pequeño, pero existen algoritmos distribuidos más eficientes para $\\scriptsize d$ grandes."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "m8483JTie_AD"
      },
      "outputs": [],
      "source": [
        "# EXERCICIO\n",
        "def pca(data, k=2):\n",
        "    \"\"\"Computes the top `k` principal components, corresponding scores, and all eigenvalues.\n",
        "\n",
        "    Note:\n",
        "        All eigenvalues should be returned in sorted order (largest to smallest). `eigh` returns\n",
        "        each eigenvectors as a column.  This function should also return eigenvectors as columns.\n",
        "\n",
        "    Args:\n",
        "        data (RDD of np.ndarray): An `RDD` consisting of NumPy arrays.\n",
        "        k (int): The number of principal components to return.\n",
        "\n",
        "    Returns:\n",
        "        tuple of (np.ndarray, RDD of np.ndarray, np.ndarray): A tuple of (eigenvectors, `RDD` of\n",
        "            scores, eigenvalues).  Eigenvectors is a multi-dimensional array where the number of\n",
        "            rows equals the length of the arrays in the input `RDD` and the number of columns equals\n",
        "            `k`.  The `RDD` of scores has the same number of rows as `data` and consists of arrays\n",
        "            of length `k`.  Eigenvalues is an array of length d (the number of features).\n",
        "    \"\"\"\n",
        "    correlatedCovAuto= estimateCovariance(data)\n",
        "    eigVals, eigVecs = np.linalg.eigh(correlatedCovAuto)\n",
        "    vals = eigVals[::-1]\n",
        "    inds1 = np.argsort(eigVals)[::-1]\n",
        "    \n",
        "    final_eigVecs1 = eigVecs[:,inds1]\n",
        "    vecs = final_eigVecs1[:,0:k]\n",
        "    \n",
        "    final_correlatedDataScores = data.map(lambda x: np.dot(x,vecs))  #Solución\n",
        "\n",
        "    # Return the `k` principal components, `k` scores, and all eigenvalues\n",
        "    return (vecs, data.map(lambda x: x.dot(vecs)), vals)\n",
        "\n",
        "# Run pca on correlatedData with k = 2\n",
        "topComponentsCorrelated, correlatedDataScoresAuto, eigenvaluesCorrelated = pca(correlatedData, 2)\n",
        "\n",
        "# Note that the 1st principal component is in the first column\n",
        "print ('topComponentsCorrelated: \\n{0}'.format(topComponentsCorrelated))\n",
        "print ('\\ncorrelatedDataScoresAuto (first three): \\n{0}'\n",
        "       .format('\\n'.join(map(str, correlatedDataScoresAuto.take(3)))))\n",
        "print ('\\neigenvaluesCorrelated: \\n{0}'.format(eigenvaluesCorrelated)\n",
        "\n",
        "# Create a higher dimensional test set\n",
        "pcaTestData = sc.parallelize([np.arange(x, x + 4) for x in np.arange(0, 20, 4)])\n",
        "componentsTest, testScores, eigenvaluesTest = pca(pcaTestData, 3)\n",
        "\n",
        "print ('\\npcaTestData: \\n{0}'.format(np.array(pcaTestData.collect()))\n",
        "print ('\\ncomponentsTest: \\n{0}'.format(componentsTest)\n",
        "print ('\\ntestScores (first three): \\n{0}'\n",
        "       .format('\\n'.join(map(str, testScores.take(3)))))\n",
        "print ('\\neigenvaluesTest: \\n{0}'.format(eigenvaluesTest)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IcekiMyde_AE"
      },
      "outputs": [],
      "source": [
        "# TEST PCA Function (2a)\n",
        "Test.assertTrue(checkBasis(topComponentsCorrelated.T,\n",
        "                           [[0.68915649,  0.72461254], [-0.72461254, 0.68915649]]),\n",
        "                'incorrect value for topComponentsCorrelated')\n",
        "firstThreeCorrelated = [[70.51682806, 69.30622356, 71.13588168], [1.48305648, 1.5888655, 1.86710679]]\n",
        "Test.assertTrue(np.allclose(firstThreeCorrelated,\n",
        "                            np.vstack(np.abs(correlatedDataScoresAuto.take(3))).T),\n",
        "                'incorrect value for firstThreeCorrelated')\n",
        "Test.assertTrue(np.allclose(eigenvaluesCorrelated, [1.94345403, 0.13820481]),\n",
        "                           'incorrect values for eigenvaluesCorrelated')\n",
        "topComponentsCorrelatedK1, correlatedDataScoresK1, eigenvaluesCorrelatedK1 = pca(correlatedData, 1)\n",
        "\n",
        "Test.assertTrue(checkBasis(topComponentsCorrelatedK1.T, [0.68915649,  0.72461254]),\n",
        "               'incorrect value for components when k=1')\n",
        "Test.assertTrue(np.allclose([70.51682806, 69.30622356, 71.13588168],\n",
        "                            np.vstack(np.abs(correlatedDataScoresK1.take(3))).T),\n",
        "                'incorrect value for scores when k=1')\n",
        "Test.assertTrue(np.allclose(eigenvaluesCorrelatedK1, [1.94345403, 0.13820481]),\n",
        "                           'incorrect values for eigenvalues when k=1')\n",
        "Test.assertTrue(checkBasis(componentsTest.T[0], [ .5, .5, .5, .5]),\n",
        "                'incorrect value for componentsTest')\n",
        "Test.assertTrue(np.allclose(np.abs(testScores.first()[0]), 3.),\n",
        "                'incorrect value for testScores')\n",
        "Test.assertTrue(np.allclose(eigenvaluesTest, [ 128, 0, 0, 0 ]), 'incorrect value for eigenvaluesTest')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BL62fhbQe_AE"
      },
      "source": [
        "#### **(2b) PCA en `dataRandom`**\n",
        "\n",
        "#### Ahora usa la función PCA para encontrar los dos componentes principales de la base esférica `dataRandom`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Edl6zddfe_AE"
      },
      "outputs": [],
      "source": [
        "# EXERCICIO\n",
        "randomData = sc.parallelize(dataRandom)\n",
        "\n",
        "# Use pca on randomData\n",
        "topComponentsRandom, randomDataScoresAuto, eigenvaluesRandom = pca(randomData, 2)      #Solución\n",
        "\n",
        "print 'topComponentsRandom: \\n{0}'.format(topComponentsRandom)\n",
        "print ('\\nrandomDataScoresAuto (first three): \\n{0}'\n",
        "       .format('\\n'.join(map(str, randomDataScoresAuto.take(3)))))\n",
        "print '\\neigenvaluesRandom: \\n{0}'.format(eigenvaluesRandom)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CCYkUWn4e_AE"
      },
      "outputs": [],
      "source": [
        "# TEST PCA on `dataRandom` (2b)\n",
        "Test.assertTrue(checkBasis(topComponentsRandom.T,\n",
        "                           [[-0.2522559 ,  0.96766056], [-0.96766056,  -0.2522559]]),\n",
        "                'incorrect value for topComponentsRandom')\n",
        "firstThreeRandom = [[36.61068572,  35.97314295,  35.59836628],\n",
        "                    [61.3489929 ,  62.08813671,  60.61390415]]\n",
        "Test.assertTrue(np.allclose(firstThreeRandom, np.vstack(np.abs(randomDataScoresAuto.take(3))).T),\n",
        "                'incorrect value for randomDataScoresAuto')\n",
        "Test.assertTrue(np.allclose(eigenvaluesRandom, [1.4204546, 0.99521397]),\n",
        "                            'incorrect value for eigenvaluesRandom')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VpqZEaPee_AE"
      },
      "source": [
        "#### **Vista 2: Proyección PCA**\n",
        "\n",
        "#### Veamos el gráfico de los datos originales y la reconstrucción unidimensional usando el componente principal para ver cómo se ve la solución PCA. Los datos de PCA se trazan en verde y las líneas representan los dos vectores de componentes principales."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ScGZwGI8e_AF"
      },
      "outputs": [],
      "source": [
        "def projectPointsAndGetLines(data, components, xRange):\n",
        "    \"\"\"Project original data onto first component and get line details for top two components.\"\"\"\n",
        "    topComponent= components[:, 0]\n",
        "    slope1, slope2 = components[1, :2] / components[0, :2]\n",
        "\n",
        "    means = data.mean()[:2]\n",
        "    demeaned = data.map(lambda v: v - means)\n",
        "    projected = demeaned.map(lambda v: (v.dot(topComponent) /\n",
        "                                        topComponent.dot(topComponent)) * topComponent)\n",
        "    remeaned = projected.map(lambda v: v + means)\n",
        "    x1,x2 = zip(*remeaned.collect())\n",
        "\n",
        "    lineStartP1X1, lineStartP1X2 = means - np.asarray([xRange, xRange * slope1])\n",
        "    lineEndP1X1, lineEndP1X2 = means + np.asarray([xRange, xRange * slope1])\n",
        "    lineStartP2X1, lineStartP2X2 = means - np.asarray([xRange, xRange * slope2])\n",
        "    lineEndP2X1, lineEndP2X2 = means + np.asarray([xRange, xRange * slope2])\n",
        "\n",
        "    return ((x1, x2), ([lineStartP1X1, lineEndP1X1], [lineStartP1X2, lineEndP1X2]),\n",
        "            ([lineStartP2X1, lineEndP2X1], [lineStartP2X2, lineEndP2X2]))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vzIqZVBhe_AF"
      },
      "outputs": [],
      "source": [
        "((x1, x2), (line1X1, line1X2), (line2X1, line2X2)) = \\\n",
        "    projectPointsAndGetLines(correlatedData, topComponentsCorrelated, 5)\n",
        "\n",
        "# generate layout and plot data\n",
        "fig, ax = preparePlot(np.arange(46, 55, 2), np.arange(46, 55, 2), figsize=(7, 7))\n",
        "ax.set_xlabel(r'Simulated $x_1$ values'), ax.set_ylabel(r'Simulated $x_2$ values')\n",
        "ax.set_xlim(45.5, 54.5), ax.set_ylim(45.5, 54.5)\n",
        "plt.plot(line1X1, line1X2, linewidth=3.0, c='#8cbfd0', linestyle='--')\n",
        "plt.plot(line2X1, line2X2, linewidth=3.0, c='#d6ebf2', linestyle='--')\n",
        "plt.scatter(dataCorrelated[:,0], dataCorrelated[:,1], s=14**2, c='#d6ebf2',\n",
        "            edgecolors='#8cbfd0', alpha=0.75)\n",
        "plt.scatter(x1, x2, s=14**2, c='#62c162', alpha=.75)\n",
        "pass"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Sn7DTVTRe_AF"
      },
      "outputs": [],
      "source": [
        "((x1, x2), (line1X1, line1X2), (line2X1, line2X2)) = \\\n",
        "    projectPointsAndGetLines(randomData, topComponentsRandom, 5)\n",
        "\n",
        "# generate layout and plot data\n",
        "fig, ax = preparePlot(np.arange(46, 55, 2), np.arange(46, 55, 2), figsize=(7, 7))\n",
        "ax.set_xlabel(r'Simulated $x_1$ values'), ax.set_ylabel(r'Simulated $x_2$ values')\n",
        "ax.set_xlim(45.5, 54.5), ax.set_ylim(45.5, 54.5)\n",
        "plt.plot(line1X1, line1X2, linewidth=3.0, c='#8cbfd0', linestyle='--')\n",
        "plt.plot(line2X1, line2X2, linewidth=3.0, c='#d6ebf2', linestyle='--')\n",
        "plt.scatter(dataRandom[:,0], dataRandom[:,1], s=14**2, c='#d6ebf2',\n",
        "            edgecolors='#8cbfd0', alpha=0.75)\n",
        "plt.scatter(x1, x2, s=14**2, c='#62c162', alpha=.75)\n",
        "pass"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qC3dxbVIe_AF"
      },
      "source": [
        "#### **(2c) Explicación de la variación**\n",
        "\n",
        "#### Finalmente, cuantifiquemos qué parte de la varianza capturó el PCA en cada uno de los datos analizados. Para ello, calcularemos la relación entre la suma de los valores propios $k$ utilizados y la suma de todos los valores propios."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZzeotE0he_AF"
      },
      "outputs": [],
      "source": [
        "# EXERCICIO\n",
        "def varianceExplained(data, k=1):\n",
        "    \"\"\"Calculate the fraction of variance explained by the top `k` eigenvectors.\n",
        "\n",
        "    Args:\n",
        "        data (RDD of np.ndarray): An RDD that contains NumPy arrays which store the\n",
        "            features for an observation.\n",
        "        k: The number of principal components to consider.\n",
        "\n",
        "    Returns:\n",
        "        float: A number between 0 and 1 representing the percentage of variance explained\n",
        "            by the top `k` eigenvectors.\n",
        "    \"\"\"\n",
        "    components, scores, eigenvalues = pca(data, k)      #Solución\n",
        "    return eigenvalues[:k].sum()/eigenvalues.sum()\n",
        "\n",
        "varianceRandom1 = varianceExplained(randomData, 1)\n",
        "varianceCorrelated1 = varianceExplained(correlatedData, 1)\n",
        "varianceRandom2 = varianceExplained(randomData, 2)\n",
        "varianceCorrelated2 = varianceExplained(correlatedData, 2)\n",
        "print ('Percentage of variance explained by the first component of randomData: {0:.1f}%'\n",
        "       .format(varianceRandom1 * 100))\n",
        "print ('Percentage of variance explained by both components of randomData: {0:.1f}%'\n",
        "       .format(varianceRandom2 * 100))\n",
        "print ('\\nPercentage of variance explained by the first component of correlatedData: {0:.1f}%'.\n",
        "       format(varianceCorrelated1 * 100))\n",
        "print ('Percentage of variance explained by both components of correlatedData: {0:.1f}%'\n",
        "       .format(varianceCorrelated2 * 100))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XCHLETLge_AG"
      },
      "outputs": [],
      "source": [
        "# TEST Variance explained (2d)\n",
        "Test.assertTrue(np.allclose(varianceRandom1, 0.588017172066), 'incorrect value for varianceRandom1')\n",
        "Test.assertTrue(np.allclose(varianceCorrelated1, 0.933608329586),\n",
        "                'incorrect value for varianceCorrelated1')\n",
        "Test.assertTrue(np.allclose(varianceRandom2, 1.0), 'incorrect value for varianceRandom2')\n",
        "Test.assertTrue(np.allclose(varianceCorrelated2, 1.0), 'incorrect value for varianceCorrelated2')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y5Gj6Xmme_AG"
      },
      "source": [
        " \n",
        "### **Parte 3: Aplicación de PCA en la base Million Song, con aplicación de Regresión Lineal en la base proyectada**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Bw9rm46ke_AG"
      },
      "source": [
        "#### **(3a) Cargando datos desde el conjunto de datos**\n",
        "\n",
        "#### Repitamos los procedimientos realizados en Lab4a para cargar y analizar la base de datos. También vamos a reescribir algunas de las funciones de utilidad de ese laboratorio."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_UWMZYmAe_AG"
      },
      "outputs": [],
      "source": [
        "baseDir = os.path.join('Data')\n",
        "inputPath = os.path.join('Aula04', 'millionsong.txt')\n",
        "fileName = os.path.join(baseDir, inputPath)\n",
        "\n",
        "numPartitions = 2\n",
        "rawData = sc.textFile(fileName, numPartitions)\n",
        "\n",
        "print rawData.first()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "N9welG5oe_AG"
      },
      "outputs": [],
      "source": [
        "from pyspark.mllib.regression import LabeledPoint\n",
        "\n",
        "def parsePoint(line):\n",
        "    \"\"\"Converts a comma separated unicode string into a `LabeledPoint`.\n",
        "\n",
        "    Args:\n",
        "        line (unicode): Comma separated unicode string where the first element is the label and the\n",
        "            remaining elements are features.\n",
        "\n",
        "    Returns:\n",
        "        LabeledPoint: The line is converted into a `LabeledPoint`, which consists of a label and\n",
        "            features.\n",
        "    \"\"\"\n",
        "    Point = map(float,line.split(','))\n",
        "    return LabeledPoint(Point[0]-1922,Point[1:])\n",
        "\n",
        "baselineRL = 17.017\n",
        "baselineInteract = 15.690\n",
        "\n",
        "millionRDD = rawData.map(parsePoint)\n",
        "print millionRDD.take(1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "E_U1HZCLe_AH"
      },
      "outputs": [],
      "source": [
        "def squaredError(label, prediction):\n",
        "    \"\"\"Calculates the the squared error for a single prediction.\n",
        "\n",
        "    Args:\n",
        "        label (float): The correct value for this observation.\n",
        "        prediction (float): The predicted value for this observation.\n",
        "\n",
        "    Returns:\n",
        "        float: The difference between the `label` and `prediction` squared.\n",
        "    \"\"\"\n",
        "    return np.square(label-prediction)\n",
        "\n",
        "def calcRMSE(labelsAndPreds):\n",
        "    \"\"\"Calculates the root mean squared error for an `RDD` of (label, prediction) tuples.\n",
        "\n",
        "    Args:\n",
        "        labelsAndPred (RDD of (float, float)): An `RDD` consisting of (label, prediction) tuples.\n",
        "\n",
        "    Returns:\n",
        "        float: The square root of the mean of the squared errors.\n",
        "    \"\"\"\n",
        "    return np.sqrt(labelsAndPreds.map(lambda rec: squaredError(rec[0],rec[1])).mean())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e1EjTjgKe_AH"
      },
      "source": [
        "#### **(3b) PCA de LabeledPoint**\n",
        "\n",
        "#### Reescribamos la función `pca`, ahora llamada `pcaLP`, para recibir un RDD de LabeledPoints. Deberá realizar dos cambios: el RDD que se pasará a `estimateCovariance` debe ser una transformación de `datos` para contener solo las `características` del LabeledPoint. La función return debe transformar solo las `características` usando los vectores propios."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7Ebj9Gfze_AH"
      },
      "outputs": [],
      "source": [
        "def pcaLP(data, k=2):\n",
        "    \"\"\"Computes the top `k` principal components, corresponding scores, and all eigenvalues.\n",
        "\n",
        "    Note:\n",
        "        All eigenvalues should be returned in sorted order (largest to smallest). `eigh` returns\n",
        "        each eigenvectors as a column.  This function should also return eigenvectors as columns.\n",
        "\n",
        "    Args:\n",
        "        data (RDD of np.ndarray): An `RDD` consisting of NumPy arrays.\n",
        "        k (int): The number of principal components to return.\n",
        "\n",
        "    Returns:\n",
        "        tuple of (np.ndarray, RDD of np.ndarray, np.ndarray): A tuple of (eigenvectors, `RDD` of\n",
        "            scores, eigenvalues).  Eigenvectors is a multi-dimensional array where the number of\n",
        "            rows equals the length of the arrays in the input `RDD` and the number of columns equals\n",
        "            `k`.  The `RDD` of scores has the same number of rows as `data` and consists of arrays\n",
        "            of length `k`.  Eigenvalues is an array of length d (the number of features).\n",
        "    \"\"\"\n",
        "    cov = estimateCovariance(data.map(lambda x: x.features))\n",
        "    eigVals, eigVecs = eigh(cov)\n",
        "    inds = np.argsort(-eigVals)\n",
        "    vecs = eigVecs[:,inds[:k]]\n",
        "    vals = eigVals[inds[:cov.shape[0]]]\n",
        "\n",
        "    # Return the `k` principal components, `k` scores, and all eigenvalues\n",
        "    return data.map(lambda x: LabeledPoint(x.label,x.features.dot(vecs)))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KE8uflCCe_AH"
      },
      "source": [
        "#### **(3c) Determinación del valor de k**\n",
        "\n",
        "#### Usemos la función `varianceExplained` aplicada a `millionRDD` (pista: primero debe realizar una transformación) para determinar el valor de k que usaremos para las pruebas de regresión lineal."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rcBFbvvye_AI"
      },
      "outputs": [],
      "source": [
        "for k in range(1,10):\n",
        "    varexp = varianceExplained(millionRDD.map(lambda x: x.features), k)\n",
        "    print 'Variation explained by {} components is {}'.format(k,varexp)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RwUmSXOWe_AI"
      },
      "source": [
        "#### **(3d) Regresión lineal**\n",
        "\n",
        "#### Apliquemos la regresión lineal del laboratorio 4a al RDD del PCA de millones de datos de canciones para k = 2, k=6 y k=8. Los resultados se compararán entre sí y con los resultados obtenidos en el Laboratorio 4a."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "dmnnuJ9Me_AI"
      },
      "outputs": [],
      "source": [
        "from pyspark.mllib.regression import LinearRegressionWithSGD\n",
        "# Values to use when training the linear regression model\n",
        "numIters = 2000  # iterations\n",
        "alpha = 1.0  # step\n",
        "miniBatchFrac = 1.0  # miniBatchFraction\n",
        "reg = 1e-1  # regParam\n",
        "regType = None#'l2'  # regType\n",
        "useIntercept = True  # intercept"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "q6V0pXGPe_AI"
      },
      "outputs": [],
      "source": [
        "# TODO: Replace <FILL IN> with appropriate code\n",
        "# Run pca using scaledData\n",
        "pcaMillionRDD = pcaLP(millionRDD, 2)\n",
        "\n",
        "weights = [.8, .1, .1]\n",
        "seed = 42\n",
        "parsedTrainData, parsedValData, parsedTestData = pcaMillionRDD.randomSplit(weights, seed)\n",
        "\n",
        "pcaModel = LinearRegressionWithSGD.train(parsedTrainData, iterations = numIters, step = alpha, miniBatchFraction = 1.0,\n",
        "                                          regParam=reg,regType=regType, intercept=useIntercept)\n",
        "labelsAndPreds = parsedValData.map(lambda lp: (lp.label, pcaModel.predict(lp.features)))\n",
        "rmseValLPCA2 = calcRMSE(labelsAndPreds)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "RiNc83Vhe_AI"
      },
      "outputs": [],
      "source": [
        "pcaMillionRDD = pcaLP(millionRDD, 6)\n",
        "\n",
        "weights = [.8, .1, .1]\n",
        "seed = 42\n",
        "parsedTrainData, parsedValData, parsedTestData = pcaMillionRDD.randomSplit(weights, seed)\n",
        "\n",
        "pcaModel = LinearRegressionWithSGD.train(parsedTrainData, iterations = numIters, step = alpha, miniBatchFraction = 1.0,\n",
        "                                          regParam=reg,regType=regType, intercept=useIntercept)\n",
        "labelsAndPreds = parsedValData.map(lambda lp: (lp.label, pcaModel.predict(lp.features)))\n",
        "rmseValLPCA6 = calcRMSE(labelsAndPreds)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "TzxdiBVEe_AJ"
      },
      "outputs": [],
      "source": [
        "pcaMillionRDD = pcaLP(millionRDD, 8)\n",
        "\n",
        "weights = [.8, .1, .1]\n",
        "seed = 42\n",
        "parsedTrainData, parsedValData, parsedTestData = pcaMillionRDD.randomSplit(weights, seed)\n",
        "\n",
        "pcaModel = LinearRegressionWithSGD.train(parsedTrainData, iterations = numIters, step = alpha, miniBatchFraction = 1.0,\n",
        "                                          regParam=reg,regType=regType, intercept=useIntercept)\n",
        "labelsAndPreds = parsedValData.map(lambda lp: (lp.label, pcaModel.predict(lp.features)))\n",
        "rmseValLPCA8 = calcRMSE(labelsAndPreds)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LFxh2J9ce_AJ"
      },
      "outputs": [],
      "source": [
        "print ('Validation RMSE:\\n\\tLinear Regression (orig.) = {0:.3f}\\n\\tLinear Regression Interact = {1:.3f}' +\n",
        "       '\\n\\tLR PCA (k=2) = {2:.3f}\\n\\tLR PCA (k=6) = {3:.3f}\\n\\tLR PCA (k=8) = {4:.3f}' \n",
        "      ).format(baselineRL, baselineInteract, rmseValLPCA2, rmseValLPCA6, rmseValLPCA8)"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.7"
    },
    "colab": {
      "name": "Lab_PCA_Resolucion.ipynb",
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}